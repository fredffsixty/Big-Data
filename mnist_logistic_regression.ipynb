{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un regressore logistico per classificare le cifre MNIST\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# Abilitiamo la eager execution: cioÃ¨ l'esecuzione immediata delle operazioni \n",
    "# che rende inutile il grafo di computazione e quindi la sessione\n",
    "# TF 1.x non la abilita di default e quindi esegue il grafo solo alla richiesta del risultato\n",
    "# La eager execution Ã¨ abilitata di default in TF 2.x\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# log dei messaggi solo  per gli errori\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Importiamo il dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Creiamo la struttura del data set per l'addestramento del modello\n",
    "def process_input_data(x_train, y_train, x_test):\n",
    "    \n",
    "    # stimiamo il numero delle classi e delle feature per creare un vettore monodimensionale\n",
    "\n",
    "    num_classes = y_train.max() + 1 # le classi hanno etichette numeriche da 0 a num_classes - 1\n",
    "\n",
    "    num_features = x_train.shape[1] * x_train.shape[2]\n",
    "    \n",
    "    # Convertiamo in float32.\n",
    "\n",
    "    x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "\n",
    "    # Convertiamo le immagini con shape (28,28) in un vettore monodimensionale di 784 elementi (Flattening)\n",
    "\n",
    "    x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "\n",
    "    # Creiamo l'embedding numerico ovvero la rappresentazione come reali in [0,1]\n",
    "\n",
    "    x_train, x_test = x_train / 255., x_test / 255.\n",
    "    \n",
    "    # il vero Dataset TF Ã¨ ottenuto per trasformazione di una serie di 'slice' di un tensore\n",
    "    # ovvero una serie di immagini in una sequenza organizzata come una matrice 3D\n",
    "    train_data=tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "    \n",
    "    return (num_classes, num_features, train_data, x_train, x_test)\n",
    "\n",
    "# Costruiamo il modello\n",
    "def build_model(learning_rate,momentum,nesterov,num_features,num_classes):\n",
    "    \n",
    "    # definiamo il nodo di bias inizializzato a 0\n",
    "    b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")\n",
    "\n",
    "    # definiamo il nodo della matrice pesi 'W' inizializzata a 1\n",
    "    W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n",
    "\n",
    "    # L'ottimizzazione viene effettuata con lo Stochastic Gradient Descent\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate,momentum=momentum,nesterov=nesterov)\n",
    "\n",
    "    return b, W, optimizer\n",
    "\n",
    " \n",
    "# Creiamo il grafo di computazione per la regressione logistica multiclasse softmax(x*W +b)\n",
    "def logistic_regression(x, W, b):\n",
    "        \n",
    "    # calcoliamo la regressione logistica y = softmax(x*W + b)\n",
    "    # x*W + b --> logit: log-probabilitÃ  non normalizzata\n",
    "    # y --> probabilitÃ  scalate in [0, 1]\n",
    "    return tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# Definiamo la funzione di loss \n",
    "\n",
    "def cross_entropy(y_pred, y_true, num_classes):\n",
    "\n",
    "    # Codifica one-hot: un vettore binario di 10 elementi con un solo valore 1\n",
    "    # per la classe predetta Es. etichetta 3 --> 0001000000\n",
    "\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "\n",
    "    # saturiamo i valori predetti per evitare il calcolo di log(0)\n",
    "\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "\n",
    "    # Calcoliamo la cross_entropia che Ã¨ l'entropia \"mutua\" tra la distribuzione di probabilitÃ \n",
    "    # p_data rappresentata dai valori y_true e la distribuzione stimata p_model\n",
    "    # rappresentata da y_pred\n",
    "    #\n",
    "    # H = - E_p_data[log p_model] --> ð¨_i (- y_true_i*log y_pred_i )\n",
    "    #\n",
    "    # Se H Ã¨ bassa allora le due distribuzioni di probabilitÃ  sono simili\n",
    "\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred),axis=1))\n",
    "\n",
    "# Definiamo la metrica di valutazione\n",
    "# usiamo l'accuracy ovvero il rapporto tra le predizioni corrette e il totale\n",
    "def accuracy(y_pred, y_true):\n",
    "\n",
    "    # La classe predetta Ã¨ l'indice dello score piÃ¹ alto nel vettore di predizione\n",
    "    # l'output della softmax Ã¨ una 'log-probabilitÃ ' ed Ã¨ quindi un numero reale\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# Definiamo la procedura di aggiornamento dei gradienti\n",
    "\n",
    "def run_optimization(x, y, W, b, optimizer, num_classes):\n",
    "\n",
    "    # Inseriamo tutta la computazione in un context manager creato da tf.GradientTape\n",
    "    # per il calcolo automatico dei gradienti\n",
    "\n",
    "    with tf.GradientTape() as g:\n",
    "\n",
    "        pred = logistic_regression(x, W, b)\n",
    "\n",
    "        loss = cross_entropy(pred, y, num_classes)\n",
    "\n",
    "    # Calcoliamo i gradienti e li applichiamo\n",
    "    # all'ottimizzatore \n",
    "    gradients = g.gradient(loss, [W, b])\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "\n",
    "\n",
    "#\n",
    "# Procedura di addestramento principale\n",
    "# \n",
    "\n",
    "def train_logisitc_regression(\\\n",
    "    x_train, y_train, x_test, y_test,\\\n",
    "    learning_rate = 0.01,\\\n",
    "    momentum=0.9,\\\n",
    "    nesterov=True,\\\n",
    "    epochs = 100,\\\n",
    "    batch_size = 64):\n",
    "    \n",
    "    \n",
    "    # Preprocessing e creazione del training set\n",
    "    num_classes, num_features, train_data, x_train, x_test = process_input_data(x_train, y_train, x_test)\n",
    "    \n",
    "        # il dataset viene ripetuto indefinitamente e impostiamo lo shuffle \n",
    "    # la diensione del batch e il prefetch dei campioni durante l'addestramento\n",
    "    train_data=train_data.repeat().\\\n",
    "                shuffle(50000).\\\n",
    "                batch(batch_size).\\\n",
    "                prefetch(tf.compat.v1.data.experimental.AUTOTUNE) # il carico Ã¨ bilanciato dinamicamente\n",
    "\n",
    "    # Istanza del modello\n",
    "    b, W, optimizer = build_model(learning_rate,momentum,nesterov,num_features,num_classes)\n",
    "    \n",
    "    # Eseguiamo il training\n",
    "    \n",
    "    num_batches = x_train.shape[0]//batch_size +1   # arrotondiamo per eccesso il numero dei batch\n",
    "                                                    # pari alla dimensione del dataset / bach_size\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        loss, acc = 0.0, 0.0\n",
    "        \n",
    "        # in ogni epoca calcoliamo preleviamo num_batches batch\n",
    "        for (batch_x, batch_y) in train_data.take(num_batches):\n",
    "\n",
    "            # Passo di ottimizzazione\n",
    "            run_optimization(batch_x, batch_y, W, b, optimizer, num_classes)\n",
    "\n",
    "            # Metriche di training\n",
    "            pred = logistic_regression(batch_x, W, b)\n",
    "\n",
    "            loss += cross_entropy(pred, batch_y, num_classes)\n",
    "\n",
    "            acc += accuracy(pred, batch_y)\n",
    "        \n",
    "        # calcoliamo i valori medi di loss e accuracy nell'epoca sui singoli batch\n",
    "        loss /= num_batches\n",
    "        acc /= num_batches\n",
    "        \n",
    "        # Metriche di test\n",
    "        pred = logistic_regression(x_test, W, b) # qui W e b sono il modello addestrato\n",
    "        \n",
    "        test_loss = cross_entropy(pred, y_test, num_classes)\n",
    "        \n",
    "        test_acc = accuracy(pred,y_test)\n",
    "\n",
    "        print(f'Epoch: {epoch:4d}, train loss: {loss:05.3f}, train accuracy: {acc:05.3f}, test loss: {test_loss:05.3f}, test accuracy: {test_acc:05.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rpirrone/virtualenvs/deeplearning/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch:    1, train loss: 0.575, train accuracy: 0.862, test loss: 0.377, test accuracy: 0.900\n",
      "Epoch:    2, train loss: 0.372, train accuracy: 0.898, test loss: 0.334, test accuracy: 0.909\n",
      "Epoch:    3, train loss: 0.341, train accuracy: 0.905, test loss: 0.316, test accuracy: 0.914\n",
      "Epoch:    4, train loss: 0.325, train accuracy: 0.909, test loss: 0.305, test accuracy: 0.916\n",
      "Epoch:    5, train loss: 0.314, train accuracy: 0.912, test loss: 0.298, test accuracy: 0.918\n",
      "Epoch:    6, train loss: 0.306, train accuracy: 0.914, test loss: 0.293, test accuracy: 0.919\n",
      "Epoch:    7, train loss: 0.300, train accuracy: 0.915, test loss: 0.289, test accuracy: 0.920\n",
      "Epoch:    8, train loss: 0.296, train accuracy: 0.917, test loss: 0.286, test accuracy: 0.921\n",
      "Epoch:    9, train loss: 0.292, train accuracy: 0.918, test loss: 0.284, test accuracy: 0.921\n",
      "Epoch:   10, train loss: 0.288, train accuracy: 0.919, test loss: 0.282, test accuracy: 0.921\n",
      "Epoch:   11, train loss: 0.285, train accuracy: 0.920, test loss: 0.280, test accuracy: 0.922\n",
      "Epoch:   12, train loss: 0.283, train accuracy: 0.921, test loss: 0.279, test accuracy: 0.922\n",
      "Epoch:   13, train loss: 0.280, train accuracy: 0.921, test loss: 0.278, test accuracy: 0.922\n",
      "Epoch:   14, train loss: 0.278, train accuracy: 0.922, test loss: 0.277, test accuracy: 0.923\n",
      "Epoch:   15, train loss: 0.277, train accuracy: 0.923, test loss: 0.276, test accuracy: 0.923\n",
      "Epoch:   16, train loss: 0.275, train accuracy: 0.923, test loss: 0.275, test accuracy: 0.923\n",
      "Epoch:   17, train loss: 0.273, train accuracy: 0.924, test loss: 0.274, test accuracy: 0.923\n",
      "Epoch:   18, train loss: 0.272, train accuracy: 0.924, test loss: 0.274, test accuracy: 0.923\n",
      "Epoch:   19, train loss: 0.271, train accuracy: 0.924, test loss: 0.273, test accuracy: 0.923\n",
      "Epoch:   20, train loss: 0.269, train accuracy: 0.925, test loss: 0.273, test accuracy: 0.923\n",
      "Epoch:   21, train loss: 0.268, train accuracy: 0.925, test loss: 0.272, test accuracy: 0.923\n",
      "Epoch:   22, train loss: 0.267, train accuracy: 0.925, test loss: 0.272, test accuracy: 0.923\n",
      "Epoch:   23, train loss: 0.266, train accuracy: 0.925, test loss: 0.271, test accuracy: 0.923\n",
      "Epoch:   24, train loss: 0.265, train accuracy: 0.926, test loss: 0.271, test accuracy: 0.923\n",
      "Epoch:   25, train loss: 0.264, train accuracy: 0.926, test loss: 0.271, test accuracy: 0.924\n",
      "Epoch:   26, train loss: 0.263, train accuracy: 0.926, test loss: 0.270, test accuracy: 0.924\n",
      "Epoch:   27, train loss: 0.263, train accuracy: 0.927, test loss: 0.270, test accuracy: 0.924\n",
      "Epoch:   28, train loss: 0.262, train accuracy: 0.927, test loss: 0.270, test accuracy: 0.924\n",
      "Epoch:   29, train loss: 0.261, train accuracy: 0.927, test loss: 0.269, test accuracy: 0.924\n",
      "Epoch:   30, train loss: 0.260, train accuracy: 0.928, test loss: 0.269, test accuracy: 0.924\n",
      "Epoch:   31, train loss: 0.260, train accuracy: 0.928, test loss: 0.269, test accuracy: 0.924\n",
      "Epoch:   32, train loss: 0.259, train accuracy: 0.928, test loss: 0.269, test accuracy: 0.924\n",
      "Epoch:   33, train loss: 0.258, train accuracy: 0.928, test loss: 0.269, test accuracy: 0.925\n",
      "Epoch:   34, train loss: 0.258, train accuracy: 0.928, test loss: 0.268, test accuracy: 0.925\n",
      "Epoch:   35, train loss: 0.257, train accuracy: 0.929, test loss: 0.268, test accuracy: 0.925\n",
      "Epoch:   36, train loss: 0.257, train accuracy: 0.929, test loss: 0.268, test accuracy: 0.925\n",
      "Epoch:   37, train loss: 0.256, train accuracy: 0.929, test loss: 0.268, test accuracy: 0.925\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/virtualenvs/deeplearning/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, name)\u001b[0m\n\u001b[1;32m   6115\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6116\u001b[0;31m         off_value, \"axis\", axis)\n\u001b[0m\u001b[1;32m   6117\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12240/523767808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_logisitc_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12240/1870110869.py\u001b[0m in \u001b[0;36mtrain_logisitc_regression\u001b[0;34m(x_train, y_train, x_test, y_test, learning_rate, momentum, nesterov, epochs, batch_size)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12240/1870110869.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(y_pred, y_true, num_classes)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# per la classe predetta Es. etichetta 3 --> 0001000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# saturiamo i valori predetti per evitare il calcolo di log(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/deeplearning/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/deeplearning/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, dtype, name)\u001b[0m\n\u001b[1;32m   3514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     return gen_array_ops.one_hot(indices, depth, on_value, off_value, axis,\n\u001b[0;32m-> 3516\u001b[0;31m                                  name)\n\u001b[0m\u001b[1;32m   3517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/deeplearning/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, name)\u001b[0m\n\u001b[1;32m   6120\u001b[0m         return one_hot_eager_fallback(\n\u001b[1;32m   6121\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moff_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6122\u001b[0;31m             ctx=_ctx)\n\u001b[0m\u001b[1;32m   6123\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6124\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/deeplearning/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mone_hot_eager_fallback\u001b[0;34m(indices, depth, on_value, off_value, axis, name, ctx)\u001b[0m\n\u001b[1;32m   6167\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_TI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6168\u001b[0m   _result = _execute.execute(b\"OneHot\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m-> 6169\u001b[0;31m                              ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m   6170\u001b[0m   _execute.record_gradient(\n\u001b[1;32m   6171\u001b[0m       \"OneHot\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m~/virtualenvs/deeplearning/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_logisitc_regression(x_train,y_train,x_test,y_test,batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8fa43838452e9e93f8e7562de70ac54321a3b66f4651442f84440771fb9be48"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('deeplearning': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
